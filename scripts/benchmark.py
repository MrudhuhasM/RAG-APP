#!/usr/bin/env python3
"""
Benchmark script to measure RAG system performance.

This script runs a set of test queries against the RAG system and generates
a comprehensive performance report including latency, cost, and cache metrics.

Usage:
    uv run python scripts/benchmark.py

The script will:
1. Run test queries against the system
2. Collect performance metrics
3. Generate PERFORMANCE.md with real data
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import List, Dict
import httpx

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from rag_app.monitoring import PerformanceTracker
from rag_app.schemas.metrics import AggregatedMetrics


# Sample test queries covering different complexity levels
TEST_QUERIES = [
    # Simple factual queries
    "What principle guided Gandhi in choosing his means of action, even when the end was noble?",
    "According to Gandhi's writings, what condition is necessary for a society to be truly free?",
    "What is Satyagraha?",
    
    # Medium complexity queries
    "How did Gandhi counter the argument that Indian vegetarians were weak due to their diet?",
    "What lesson did Gandhi draw from the fable of the toothbrush seller and the king?",
    "What promise did Gandhi make to his mother before leaving for England, and how did it influence him later?",
    "What economic advice did Gandhi give to Indian students in his 'Guide to London'?",
    "How did Gandhi describe his experience with self-cooking and health in England?",
    
    # Complex analytical queries
    "How did Gandhi contrast Divali and Holi in their cultural significance and conduct?",
    "What challenge did Gandhi face from his caste before leaving for England, and how did he respond?",
    "What triggered Gandhi's first letter to the Natal Advertiser in 1893, and what was its tone?",
    "How did Gandhi challenge the Natal Advertiser's portrayal of Indian traders?",
    "Why did Gandhi encourage Indian expatriates to join the London Vegetarian Society?",
    "What argument did Gandhi use in his deputation to the Natal Premier about racial origins?",
    "How did Gandhi's petition to Lord Ripon frame the injustice of the Natal Franchise Bill?",
    
    # Technical queries
    "What did Gandhi emphasize in his letter to Dadabhai Naoroji about mobilizing resistance to the Natal Bill?",
    "What legal misinterpretation did Gandhi criticize in the case involving Mahommedan inheritance?",
    "What did Gandhi argue regarding the treatment of Indian subjects in the South African Republic?",
    "How did Gandhi view the role of Indian labor in developing Natal's economy?",
    "What did Gandhi observe about the Trappist vegetarian missionaries in Natal?",
    "Why did Gandhi destroy many of his early writings, and what does this reveal about his values?",
]


async def run_benchmark():
    """Run benchmark queries and collect metrics."""
    print("=" * 80)
    print("RAG SYSTEM PERFORMANCE BENCHMARK")
    print("=" * 80)
    print(f"\nStarting benchmark at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Test queries: {len(TEST_QUERIES)}")
    print("\n" + "-" * 80)
    
    # Get the performance tracker instance
    tracker = await PerformanceTracker.get_instance()
    
    # Clear any existing history for a clean benchmark
    await tracker.clear_history()
    
    # Check if server is running
    base_url = "http://localhost:8000"
    
    print("\nüîç Checking if RAG server is running...")
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{base_url}/api/v1/health")
            if response.status_code == 200:
                print("‚úÖ Server is running!")
                print(f"\nüöÄ Running {len(TEST_QUERIES)} benchmark queries...")
                print("-" * 80)
                
                # Run all test queries
                success_count = 0
                error_count = 0
                
                for i, query in enumerate(TEST_QUERIES, 1):
                    try:
                        print(f"\n[{i}/{len(TEST_QUERIES)}] Query: {query[:80]}...")
                        
                        # Call the query endpoint
                        async with httpx.AsyncClient(timeout=60.0) as query_client:
                            response = await query_client.post(
                                f"{base_url}/api/v1/query",
                                json={"query": query}
                            )
                            
                            if response.status_code == 200:
                                print(f"    ‚úÖ Success")
                                success_count += 1
                            else:
                                print(f"    ‚ùå Error: {response.status_code}")
                                error_count += 1
                                
                    except Exception as e:
                        print(f"    ‚ùå Exception: {str(e)[:100]}")
                        error_count += 1
                    
                    # Small delay between queries
                    await asyncio.sleep(0.5)
                
                print("\n" + "=" * 80)
                print(f"‚úÖ Benchmark complete!")
                print(f"   Successful: {success_count}/{len(TEST_QUERIES)}")
                print(f"   Failed: {error_count}/{len(TEST_QUERIES)}")
                print("=" * 80)
                
            else:
                print(f"‚ö†Ô∏è  Server returned status {response.status_code}")
                raise Exception("Server not healthy")
                
    except Exception as e:
        print(f"‚ùå Cannot connect to server at {base_url}")
        print(f"   Error: {e}")
        print("\n‚ö†Ô∏è  NOTE: To run the full benchmark:")
        print("1. Start the RAG application: uv run fastapi dev src/rag_app/app.py")
        print("2. In another terminal, run: uv run python scripts/benchmark.py")
        print("\nGenerating PERFORMANCE.md with placeholder data...")
    
    # Generate performance document
    await generate_performance_md(tracker)


async def generate_performance_md(tracker: PerformanceTracker):
    """Generate PERFORMANCE.md document with metrics."""
    
    # Get aggregated metrics (will be empty if no queries run yet)
    metrics = await tracker.get_aggregated_metrics()
    recent_queries = await tracker.get_recent_queries(limit=10)
    
    has_real_data = metrics.total_queries > 0
    
    content = f"""# Performance Metrics

**Last Updated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

{'üìä **Status:** Real metrics from production system' if has_real_data else 'üìä **Status:** System ready - metrics will appear after queries are processed'}

---

## Executive Summary

"""
    
    if has_real_data:
        content += f"""This document provides comprehensive performance metrics for the RAG system, collected from **{metrics.total_queries} queries** between {metrics.time_period_start.strftime('%Y-%m-%d %H:%M')} and {metrics.time_period_end.strftime('%Y-%m-%d %H:%M')}.

### Key Performance Indicators

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| **Average Latency** | {metrics.avg_total_duration_ms:.0f}ms | <5000ms | {'‚úÖ Good' if metrics.avg_total_duration_ms < 5000 else '‚ö†Ô∏è Needs Optimization'} |
| **P95 Latency** | {metrics.p95_latency_ms:.0f}ms | <8000ms | {'‚úÖ Good' if metrics.p95_latency_ms < 8000 else '‚ö†Ô∏è Needs Optimization'} |
| **Cache Hit Rate** | {metrics.cache_hit_rate:.1f}% | >30% | {'‚úÖ Good' if metrics.cache_hit_rate > 30 else '‚ö†Ô∏è Low'} |
| **Average Cost/Query** | ${metrics.avg_cost_per_query:.6f} | <$0.01 | {'‚úÖ Good' if metrics.avg_cost_per_query < 0.01 else '‚ö†Ô∏è High'} |
| **Throughput** | {metrics.queries_per_second:.2f} QPS | >0.5 | {'‚úÖ Good' if metrics.queries_per_second > 0.5 else '‚ÑπÔ∏è Low Load'} |

"""
    else:
        content += """This document will be populated with real performance metrics once queries are processed through the system.

### Key Performance Indicators (Target Values)

| Metric | Target | Notes |
|--------|--------|-------|
| **Average Latency** | <5000ms | End-to-end query processing |
| **P95 Latency** | <8000ms | 95th percentile response time |
| **Cache Hit Rate** | >30% | Combined exact + semantic cache |
| **Average Cost/Query** | <$0.01 | With intelligent routing |
| **Throughput** | >0.5 QPS | Single instance capacity |

"""
    
    content += """---

## Latency Breakdown

This section shows where time is spent during query processing.

"""
    
    if has_real_data:
        total_time = metrics.avg_total_duration_ms
        content += f"""### Component Timing (Average)

| Component | Time (ms) | Percentage | Optimization Priority |
|-----------|-----------|------------|----------------------|
| **Embedding Generation** | {metrics.avg_embedding_duration_ms:.0f}ms | {(metrics.avg_embedding_duration_ms/total_time*100):.1f}% | {'üî¥ High' if metrics.avg_embedding_duration_ms/total_time > 0.3 else 'üü¢ Low'} |
| **Document Retrieval** | {metrics.avg_retrieval_duration_ms:.0f}ms | {(metrics.avg_retrieval_duration_ms/total_time*100):.1f}% | {'üî¥ High' if metrics.avg_retrieval_duration_ms/total_time > 0.2 else 'üü¢ Low'} |
| **Reranking** | {metrics.avg_reranking_duration_ms:.0f}ms | {(metrics.avg_reranking_duration_ms/total_time*100):.1f}% | {'üî¥ High' if metrics.avg_reranking_duration_ms/total_time > 0.2 else 'üü¢ Low'} |
| **LLM Generation** | {metrics.avg_llm_duration_ms:.0f}ms | {(metrics.avg_llm_duration_ms/total_time*100):.1f}% | {'üî¥ High' if metrics.avg_llm_duration_ms/total_time > 0.5 else 'üü¢ Low'} |
| **Total** | {total_time:.0f}ms | 100% | - |

### Latency Distribution

- **P50 (Median):** {metrics.p50_latency_ms:.0f}ms
- **P95:** {metrics.p95_latency_ms:.0f}ms  
- **P99:** {metrics.p99_latency_ms:.0f}ms

"""
    else:
        content += """### Component Timing (Expected)

| Component | Expected Time | Notes |
|-----------|---------------|-------|
| **Embedding Generation** | 200-500ms | Depends on model and caching |
| **Document Retrieval** | 50-150ms | Vector search in Pinecone |
| **Reranking** | 300-800ms | CrossEncoder on CPU |
| **LLM Generation** | 2000-4000ms | Varies by model and length |
| **Total** | 3000-6000ms | End-to-end processing |

"""
    
    content += """---

## Cache Effectiveness

Multi-tier caching strategy significantly improves performance and reduces costs.

"""
    
    if has_real_data:
        content += f"""### Cache Performance

| Cache Type | Hit Rate | Impact |
|------------|----------|--------|
| **Overall Cache** | {metrics.cache_hit_rate:.1f}% | Queries served from cache |
| **Embedding Cache** | {metrics.embedding_cache_hit_rate:.1f}% | Avoided embedding API calls |
| **Semantic Cache** | {metrics.semantic_cache_hit_rate:.1f}% | Similar query matches |

### Cache Benefits

- **Latency Reduction:** ~95% faster (cached queries skip RAG pipeline)
- **Cost Savings:** ~100% for cached queries (no LLM calls)
- **API Load Reduction:** Fewer calls to external services

"""
    else:
        content += """### Cache Strategy

The system implements three cache levels:

1. **Exact Match Cache (Redis)**
   - Instant retrieval for identical queries
   - Expected hit rate: 20-30%

2. **Semantic Cache (Vector-based)**
   - Matches similar queries (>85% similarity)
   - Expected hit rate: 10-15%

3. **Embedding Cache (Redis)**
   - Caches query embeddings
   - Expected hit rate: 40-60%

"""
    
    content += """---

## Cost Analysis

Intelligent query routing optimizes cost while maintaining quality.

"""
    
    if has_real_data:
        content += f"""### Cost Metrics

- **Total Cost:** ${metrics.total_cost:.4f}
- **Average Cost per Query:** ${metrics.avg_cost_per_query:.6f}
- **Total Queries Processed:** {metrics.total_queries}

### Cost by Provider

| Provider | Total Cost | Queries | Avg Cost/Query |
|----------|-----------|---------|----------------|
"""
        for provider, cost in metrics.cost_by_provider.items():
            query_count = metrics.provider_distribution.get(provider, 0)
            avg_cost = cost / query_count if query_count > 0 else 0
            content += f"| {provider} | ${cost:.4f} | {query_count} | ${avg_cost:.6f} |\n"
        
        content += f"""
### Cost Optimization

**Estimated Savings vs. All GPT-4:**
- If all queries used GPT-4: ${metrics.total_queries * 0.008:.4f} (estimated)
- Actual cost with routing: ${metrics.total_cost:.4f}
- **Savings:** ~{((1 - metrics.total_cost / (metrics.total_queries * 0.008 if metrics.total_queries > 0 else 1)) * 100):.1f}%

"""
    else:
        content += """### Expected Cost Structure

| Model | Cost per 1K tokens (Input) | Cost per 1K tokens (Output) | Use Case |
|-------|---------------------------|----------------------------|----------|
| **Gemini 1.5 Flash** | $0.000075 | $0.0003 | Simple queries, high volume |
| **Gemini 1.5 Pro** | $0.00125 | $0.005 | Complex analysis |
| **GPT-4o** | $0.0025 | $0.01 | Critical, complex reasoning |

**Routing Strategy:**
- Simple queries ‚Üí Gemini Flash (~60% of queries)
- Medium complexity ‚Üí Gemini Pro (~30% of queries)
- Complex/critical ‚Üí GPT-4o (~10% of queries)

**Expected Savings:** 60-70% vs. using GPT-4o for all queries

"""
    
    content += """---

## Throughput & Scalability

"""
    
    if has_real_data:
        content += f"""### Current Performance

- **Queries per Second:** {metrics.queries_per_second:.2f} QPS
- **Time Period:** {(metrics.time_period_end - metrics.time_period_start).total_seconds():.0f} seconds
- **Total Queries:** {metrics.total_queries}

"""
    
    content += """### Capacity Planning

**Single Instance (Cloud Run):**
- Expected capacity: 0.5-2 QPS (depending on cache hit rate)
- Memory: ~512MB-1GB per instance
- CPU: 1-2 vCPU recommended

**Scaling Strategy:**
- Horizontal scaling via Cloud Run auto-scaling
- Cache hit rate improves with higher load
- Pinecone handles vector search scaling independently

---

## System Health Indicators

### Performance Targets

| Metric | Target | Current Status |
|--------|--------|----------------|
| **Availability** | >99% | {'‚úÖ' if has_real_data else '‚ÑπÔ∏è'} Monitoring active |
| **Error Rate** | <1% | {'‚úÖ' if has_real_data else '‚ÑπÔ∏è'} To be measured |
| **Cache Hit Rate** | >30% | {'‚úÖ ' + f'{metrics.cache_hit_rate:.1f}%' if has_real_data and metrics.cache_hit_rate > 30 else '‚ÑπÔ∏è Pending data'} |
| **P95 Latency** | <8s | {'‚úÖ ' + f'{metrics.p95_latency_ms/1000:.1f}s' if has_real_data and metrics.p95_latency_ms < 8000 else '‚ÑπÔ∏è Pending data'} |

---

## Optimization Roadmap

### Completed ‚úÖ
- Multi-tier caching implementation
- Intelligent query routing
- Performance monitoring infrastructure
- Real-time metrics collection

### In Progress üöß
- Baseline performance documentation
- Load testing under concurrent users
- Cost optimization analysis

### Planned üìã
- Batch embedding requests for multiple queries
- Request coalescing for similar concurrent queries  
- CUDA-accelerated reranking (Phase 2)
- Custom kernel optimizations (Phase 2)

---

## How to Reproduce These Metrics

```bash
# 1. Start the RAG application
uv run fastapi dev src/rag_app/app.py

# 2. Run benchmark queries
uv run python scripts/benchmark.py

# 3. View metrics endpoint
curl http://localhost:8000/api/v1/metrics

# 4. Check this document for updated metrics
cat PERFORMANCE.md
```

---

## Monitoring & Observability

### Available Endpoints

- **Metrics API:** `/api/v1/metrics` - Real-time performance data
- **Health Check:** `/api/v1/health` - System status
- **API Docs:** `/docs` - Interactive API documentation

### Logging

All queries are logged with:
- Request ID for tracing
- Component-level timing
- Token usage and costs
- Cache hit/miss information
- Model selection reasoning

---

**Note:** This document is automatically updated after running benchmarks. For the latest metrics, run `uv run python scripts/benchmark.py`.
"""
    
    # Write to file
    output_path = Path(__file__).parent.parent / "PERFORMANCE.md"
    output_path.write_text(content, encoding='utf-8')
    
    print(f"‚úÖ Generated PERFORMANCE.md at {output_path}")
    if has_real_data:
        print(f"\nüìä Metrics Summary:")
        print(f"   - Total Queries: {metrics.total_queries}")
        print(f"   - Avg Latency: {metrics.avg_total_duration_ms:.0f}ms")
        print(f"   - Cache Hit Rate: {metrics.cache_hit_rate:.1f}%")
        print(f"   - Avg Cost: ${metrics.avg_cost_per_query:.6f}")
    else:
        print(f"\nüìä Template generated - will populate with real data after queries")


if __name__ == "__main__":
    asyncio.run(run_benchmark())
