services:
  rag-app:
    build:
      context: .
      dockerfile: Dockerfile
    image: rag-app:latest
    container_name: rag-app
    ports:
      - "8000:8000"
    environment:
      # Environment
      - ENV=production
      - LOG_LEVEL=INFO
      
      # Application Settings
      - APP_NAME=RAG App
      - APP_VERSION=0.1.0
      - MAX_FILE_SIZE=10485760
      - CORS_ALLOWED_ORIGINS=*
      
      # HTTP Timeouts
      - HTTP_TIMEOUT=30.0
      - EMBEDDING_TIMEOUT=20.0
      
      # OpenAI Configuration
      - OPENAI__API_KEY=${OPENAI_API_KEY:-}
      - OPENAI__EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL:-}
      - OPENAI__COMPLETION_MODEL=${OPENAI_COMPLETION_MODEL:-}
      
      # Gemini Configuration (Optional)
      - GEMINI__API_KEY=${GEMINI_API_KEY:-}
      - GEMINI__EMBEDDING_MODEL=${GEMINI_EMBEDDING_MODEL:-}
      - GEMINI__COMPLETION_MODEL=${GEMINI_COMPLETION_MODEL:-}
      
      # Provider Selection
      - EMBEDDING__PROVIDER=${EMBEDDING_PROVIDER:-gemini}
      - LLM__PROVIDER=${LLM_PROVIDER:-gemini}
      
      # Pinecone Configuration
      - PINECONE__API_KEY=${PINECONE_API_KEY}
      - PINECONE__INDEX_NAME=${PINECONE_INDEX_NAME:-rag-index}
      - PINECONE__DIMENSION=${PINECONE_DIMENSION:-1536}
      - PINECONE__METRIC=${PINECONE_METRIC:-cosine}
      - PINECONE__CLOUD=${PINECONE_CLOUD:-aws}
      - PINECONE__REGION=${PINECONE_REGION:-us-east-1}
      - PINECONE__ENVIRONMENT=${PINECONE_ENVIRONMENT:-production}
      - PINECONE__QUESTIONS_NAMESPACE=${PINECONE_QUESTIONS_NAMESPACE:-questions}
      
      # Reranker Model
      - RERANKER__MODEL=${RERANKER_MODEL:-BAAI/bge-reranker-base}
    
    volumes:
      # Mount logs directory for persistence
      - ./logs:/app/logs
    
    restart: unless-stopped
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge
